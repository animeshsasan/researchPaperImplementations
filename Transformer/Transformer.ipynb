{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b7b380a-40eb-4d99-988d-89f0d3b9202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8612ad63-f8b4-4959-a4cd-beae176d4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "train_split = 0.9\n",
    "test_split = 1 - train_split\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "320ad23c-3df1-4f01-ba30-7dd7404aea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efd106cd-015c-4947-99c3-e82e3e422b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(train_split*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "834b430f-8f3e-4546-9bdd-632df2c12c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding \n",
    "    PE(pos,2i) =sin(pos/10000^(2i/dmodel))\n",
    "    PE(pos,2i+1) =cos(pos/10000^(2i/dmodel))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        \"\"\"\n",
    "        pos: (seq_length, 1)\n",
    "        i: (1, d_model)\n",
    "        d_model: int (dimension of embedding)\n",
    "\n",
    "        return: (seq_length, d_model)\n",
    "        \"\"\"\n",
    "        power = 2*(i//2)/ torch.tensor(d_model, dtype=torch.float32)\n",
    "        return pos / (torch.pow(10000, power))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        assert len(inputs.shape) == 3\n",
    "        seq_length = inputs.shape[-2]\n",
    "        d_model = inputs.shape[-1]\n",
    "        angles = self.get_angles(\n",
    "            torch.arange(seq_length).unsqueeze(1),\n",
    "            torch.arange(d_model).unsqueeze(0),\n",
    "            d_model\n",
    "        )\n",
    "        \n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        pe[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "        pe.unsqueeze(0)\n",
    "        return inputs + pe\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "420298df-66ea-40d2-b286-384cc9db0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Positional encoding test\n",
    "\"\"\"\n",
    "test_pe_input = torch.tensor([\n",
    "    [[1,2,3], [2,3,4]], \n",
    "    [[3,4,5], [4,5,6]]\n",
    "]) #batch_size = 2, seq_length = 2, d_model = 3\n",
    "pos1_i0 = torch.sin(torch.tensor(1/math.pow(10000,0)))\n",
    "pos1_i1 = torch.cos(torch.tensor(1/math.pow(10000,0)))\n",
    "pos1_i2 = torch.sin(torch.tensor(1/math.pow(10000,2/float(3))))\n",
    "expected_pe = torch.tensor([\n",
    "    [[0, 1, 0], [pos1_i0, pos1_i1, pos1_i2]], \n",
    "    [[0, 1, 0], [pos1_i0, pos1_i1, pos1_i2]]\n",
    "]) + test_pe_input\n",
    "assert (expected_pe == PositionalEncoding().forward(test_pe_input)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "697f72bb-7913-4d70-9231-d01e9e9d2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, input_shape):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        self.query_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model)\n",
    "        self.key_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model)\n",
    "        self.value_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model)\n",
    "        self.final_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model)\n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        softmax((QK.T)/sqrt(dk))V\n",
    "        \n",
    "        query: (batch_size, num_heads, seq_length, d_k)\n",
    "        key: (batch_size, num_heads, seq_length, d_k)\n",
    "        value: (batch_size, num_heads, seq_length, d_v)\n",
    "        mask: (batch_size, 1, 1, seq_length)\n",
    "        return: (batch_size, num_heads, seq_length, d_v)\n",
    "        \"\"\"\n",
    "        assert len(query.shape) == len(key.shape) and len(query.shape) == len(value.shape)\n",
    "        assert key.dtype == torch.float\n",
    "        \n",
    "        product = query @ (key.transpose(-1,-2))\n",
    "        \n",
    "        dk = torch.tensor(key.shape[-1], dtype = torch.float32)\n",
    "        sqrt_dk = torch.sqrt(dk)\n",
    "        scaled_product = product/sqrt_dk\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_product += mask * -1e9\n",
    "\n",
    "        softmax = torch.softmax(scaled_product, dim = -1)\n",
    "        attention = softmax @ value\n",
    "        return attention\n",
    "\n",
    "    def split_to_heads(self, inputs, batch_size):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_length, d_model)\n",
    "        return: (batch_size, n_proj, seq_length, d_model//n_heads)\n",
    "        \"\"\"\n",
    "        proj_inputs = inputs.view(batch_size, -1, self.n_heads, self.d_head)\n",
    "        return proj_inputs.transpose(1, 2)\n",
    "\n",
    "    def concat_from_heads(self, inputs, batch_size):\n",
    "        \"\"\"\n",
    "        input: (batch_size, n_proj, seq_length, d_model//n_heads)\n",
    "        return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return inputs.transpose(2,1).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query: (batch_size, seq_length, d_model)\n",
    "        key: (batch_size, seq_length, d_model)\n",
    "        value: (batch_size, seq_length, d_model)\n",
    "        mask: (batch_size, 1, 1, seq_length)\n",
    "        \n",
    "        return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "        queries = self.query_lin(query)\n",
    "        keys = self.key_lin(key)\n",
    "        values = self.value_lin(value)\n",
    "\n",
    "        queries = self.split_to_heads(queries, batch_size)\n",
    "        keys = self.split_to_heads(keys, batch_size)\n",
    "        values = self.split_to_heads(values, batch_size)\n",
    "\n",
    "        attention = self.scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        attention = self.concat_from_heads(attention, batch_size)\n",
    "        outputs = self.final_lin(attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "be5a7e40-bf19-436c-8002-aed556e2afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Head Attention Test\n",
    "\"\"\"\n",
    "def test_multi_head_attention_output_shape(input_shape = (32, 50, 64), n_heads = 8):\n",
    "     # (batch_size, seq_length, d_model)\n",
    "    mha = MultiHeadAttention(n_heads=n_heads, input_shape=input_shape)\n",
    "    batch_size, seq_length, d_model = input_shape\n",
    "    query = torch.rand(batch_size, seq_length, d_model)\n",
    "    key = torch.rand(batch_size, seq_length, d_model)\n",
    "    value = torch.rand(batch_size, seq_length, d_model)\n",
    "    mask = torch.ones(batch_size, 1, 1, seq_length)\n",
    "    outputs = mha.forward(query, key, value, mask)\n",
    "    assert outputs.shape == (batch_size, seq_length, d_model)\n",
    "    \n",
    "test_multi_head_attention_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f3c07a71-1a5e-488c-a141-bc1422da6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scaled Dot Product Attention Test\n",
    "\"\"\"\n",
    "def test_scaled_dot_product():\n",
    "    test_scaled_dot_attention = torch.tensor([\n",
    "        [[1,2,3], [2,3,4]], \n",
    "        [[3,4,5], [4,5,6]]\n",
    "    ], dtype = torch.float32) #batch_size:2, seq_length: 2, d_model: 3\n",
    "    test_mask = torch.tensor([\n",
    "        [[0, 0]],\n",
    "        [[0, 1]]\n",
    "    ])# (batch_size, 1, seq_length)\n",
    "    test_product = torch.tensor([[[14., 20.],\n",
    "             [20., 29.]],\n",
    "            [[50., 62.],\n",
    "             [62., 77.]]], dtype = torch.float32)\n",
    "    test_scaled_product = test_product/math.sqrt(3)\n",
    "    test_scaled_product[1, :, 1] += -1e9 #applying mask\n",
    "    expected_attention = torch.softmax(test_scaled_product, dim = -1) @ test_scaled_dot_attention\n",
    "    assert (expected_attention == MultiHeadAttention(5, test_scaled_dot_attention.shape).scaled_dot_product_attention(test_scaled_dot_attention, test_scaled_dot_attention, test_scaled_dot_attention, test_mask)).all()\n",
    "\n",
    "test_scaled_dot_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "3d2dec44-e15c-4655-964e-8f301211e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_shape, dff):\n",
    "        super().__init__()\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.feed_forward_inner_lin = nn.Linear(in_features = self.d_model, out_features = dff)\n",
    "        self.feed_forward_relu = nn.ReLU()\n",
    "        self.feed_forward_outer_lin = nn.Linear(in_features = dff, out_features = self.d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, seq_length, d_model)\n",
    "        return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        outputs = self.feed_forward_inner_lin(inputs)\n",
    "        outputs = self.feed_forward_relu(outputs)\n",
    "        outputs = self.feed_forward_outer_lin(outputs)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "76cd4ef5-7b5e-4f20-a135-b67f90be3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feed Forward Test\n",
    "\"\"\"\n",
    "def test_feed_forward_output_shape(input_shape = (32, 50, 64), dff = 2048):\n",
    "     # (batch_size, seq_length, d_model)\n",
    "    ffn = FeedForward(input_shape ,dff)\n",
    "    batch_size, seq_length, d_model = input_shape\n",
    "    inputs = torch.rand(batch_size, seq_length, d_model)\n",
    "    outputs = ffn.forward(inputs)\n",
    "    assert outputs.shape == (batch_size, seq_length, d_model)\n",
    "    \n",
    "test_feed_forward_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0caf9ecc-42b5-4618-861f-fecf7c044f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, input_shape, n_heads, dff, dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads=n_heads, input_shape=input_shape)\n",
    "        self.attention_dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(input_shape)\n",
    "        self.feed_forward = FeedForward(input_shape, dff)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(input_shape)\n",
    "        self.feed_forward_dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                           inputs,\n",
    "                                           inputs,\n",
    "                                           mask)\n",
    "        attention = self.attention_dropout(attention)\n",
    "        attention = self.attention_layer_norm(inputs + attention)\n",
    "\n",
    "        outputs = self.feed_forward(attention)\n",
    "        outpus = self.feed_forward_dropout(outputs)\n",
    "        outputs = self.feed_forward_layer_norm(attention + outputs)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e0ceaa23-64c0-4860-8a97-5051852b5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder Layer Test\n",
    "\"\"\"\n",
    "def test_encoder_layer_output_shape(input_shape = (32, 50, 64), n_heads = 8, dff = 2048):\n",
    "     # (batch_size, seq_length, d_model)\n",
    "    encoder_layer = EncoderLayer(input_shape = input_shape , n_heads = n_heads, dff = dff)\n",
    "    batch_size, seq_length, d_model = input_shape\n",
    "    inputs = torch.rand(batch_size, seq_length, d_model)\n",
    "    mask = torch.ones(batch_size, 1, 1, seq_length)\n",
    "    outputs = encoder_layer.forward(inputs, mask)\n",
    "    assert outputs.shape == (batch_size, seq_length, d_model)\n",
    "    \n",
    "test_encoder_layer_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f6257d3d-9088-415c-bcfc-1645839bfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape, \n",
    "                 n_heads, \n",
    "                 dff = 2048,\n",
    "                 n_layers = 6, \n",
    "                 dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = input_shape\n",
    "        self.encoder_layers = [EncoderLayer(input_shape, n_heads, dff, dropout_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        outputs = self.encoder_layers[0](inputs, mask)\n",
    "        for i in range(1, self.n_layes):\n",
    "            outputs = self.encoder_layers[i](outputs, mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "373981a3-3ec7-483c-9975-2ecde1c7a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, input_shape, n_heads, dff, dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.multi_head_masked_attention = MultiHeadAttention(n_heads=n_heads, input_shape=input_shape)\n",
    "        self.masked_attention_dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.masked_attention_layer_norm = nn.LayerNorm(input_shape)\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads=n_heads, input_shape=input_shape)\n",
    "        self.attention_dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(input_shape)\n",
    "        \n",
    "        self.feed_forward = FeedForward(input_shape, dff)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(input_shape)\n",
    "        self.feed_forward_dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, enc_outputs, mask_1, mask_2):\n",
    "        attention = self.multi_head_masked_attention(inputs,\n",
    "                                           inputs,\n",
    "                                           inputs,\n",
    "                                           mask_1)\n",
    "        attention = self.masked_attention_dropout(attention)\n",
    "        attention = self.maked_attention_layer_norm(inputs + attention)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention(attention,\n",
    "                                           enc_inputs,\n",
    "                                           enc_inputs,\n",
    "                                           mask_2)\n",
    "        attention_2 = self.attention_dropout(attention_2)\n",
    "        attention_2 = self.attention_layer_norm(attention + attention_2)\n",
    "\n",
    "        outputs = self.feed_forward(attention)\n",
    "        outpus = self.feed_forward_dropout(outputs)\n",
    "        outputs = self.feed_forward_layer_norm(attention_2 + outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d1cc5-431a-4484-a53c-0bd414c5b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape, \n",
    "                 n_heads, \n",
    "                 dff = 2048,\n",
    "                 n_layers = 6, \n",
    "                 dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = input_shape\n",
    "        self.decoder_layers = [DecoderLayer(input_shape, n_heads, dff, dropout_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def forward(self, inputs, enc_outputs, mask_1, mask_2):\n",
    "        outputs = self.decoder_layers[0](inputs, enc_outputs, mask_1, mask_2)\n",
    "        for i in range(1, self.n_layes):\n",
    "            outputs = self.decoder_layers[i](outputs, enc_outputs, mask_1, mask_2)\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
