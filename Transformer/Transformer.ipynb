{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b7b380a-40eb-4d99-988d-89f0d3b9202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8612ad63-f8b4-4959-a4cd-beae176d4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "train_split = 0.9\n",
    "test_split = 1 - train_split\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "# block_size = 32 # what is the maximum context length for predictions?\n",
    "eval_iters = 200\n",
    "\n",
    "VOCAB_SIZE = 3700\n",
    "D_MODEL = 512\n",
    "N_HEADS = 8\n",
    "DFF = 2048\n",
    "N_LAYERS = 6\n",
    "DROPOUT_RATE = 0.5\n",
    "path_to_data = './dataset/archive/'\n",
    "test_data_file = 'wmt14_translate_de-en_test.csv'\n",
    "train_data_file ='wmt14_translate_de-en_train.csv'\n",
    "validation_data_file ='wmt14_translate_de-en_validation.csv'\n",
    "path_to_tokenizer = './tokenizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0debd9a-7d48-4677-aa0a-9e29cd72b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000  # Number of rows per chunk\n",
    "num_chunks_to_read = 444  # Number of chunks to read\n",
    "\n",
    "# Initialize an empty list to store chunks\n",
    "chunks = []\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "for chunk in pd.read_csv(path_to_data + train_data_file, chunksize=chunksize):\n",
    "    chunks.append(chunk)\n",
    "    if len(chunks) >= num_chunks_to_read:\n",
    "        break\n",
    "\n",
    "# Concatenate chunks into a single DataFrame\n",
    "train_data = pd.concat(chunks, ignore_index=True)\n",
    "english_train = train_data.values[:, 1]\n",
    "german_train = train_data.values[:, 0]\n",
    "val_data = pd.read_csv(path_to_data + validation_data_file)\n",
    "english_validation = val_data.values[:, 1]\n",
    "german_validation = val_data.values[:, 0]\n",
    "test_data = pd.read_csv(path_to_data + test_data_file)\n",
    "english_test = test_data.values[:, 1]\n",
    "german_test = test_data.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8feaf903-3735-4eaa-826e-5f788a67b5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Consequently, they will be particularly motivated playing against their former coach.',\n",
       " 'Ursprünglich war die Schulhofsanierung sogar schon in den Jahren 2008/2009 geplant, doch hohe unplanmäßige Ausgaben brachten eine Verschiebung.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{test_data.loc[i,x] for i, x in enumerate(test_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6931d611-0fc5-44ea-b202-aa8ff8b9e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_complete_data(data):\n",
    "    # Identify rows with NaN values in either column\n",
    "    nan_indices = data[data.isna().any(axis=1)].index\n",
    "    \n",
    "    # Remove these rows from the DataFrame\n",
    "    cleaned_df = data.drop(nan_indices)\n",
    "    \n",
    "    return cleaned_df\n",
    "train_data = clean_complete_data(train_data)\n",
    "val_data = clean_complete_data(val_data)\n",
    "test_data = clean_complete_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76bd505-1b0b-4eed-87c2-a40b98cc2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(english, german):\n",
    "    # Find indices of NaN values in both lists\n",
    "    nan_indices_german = {i for i, x in enumerate(german) if isinstance(x, float) and math.isnan(x)}\n",
    "    nan_indices_english = {i for i, x in enumerate(english) if isinstance(x, float) and math.isnan(x)}\n",
    "    \n",
    "    nan_indices = nan_indices_german.union(nan_indices_english)\n",
    "\n",
    "    # Remove elements at NaN indices from both lists\n",
    "    english_cleaned = [x for i, x in enumerate(english) if i not in nan_indices]\n",
    "    german_cleaned = [x for i, x in enumerate(german) if i not in nan_indices]\n",
    "    return english_cleaned, german_cleaned\n",
    "english_train, german_train = clean_data(english_train, german_train)\n",
    "english_test, german_test = clean_data(english_test, german_test)\n",
    "english_validation, german_validation = clean_data(english_validation, german_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320ad23c-3df1-4f01-ba30-7dd7404aea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    shakespear_data = f.read()\n",
    "\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# stoi = {s:i for i,s in enume rate(chars)}\n",
    "# itos = {i:s for i,s in enumerate(chars)}\n",
    "# encode = lambda s: [stoi[c] for c in s]\n",
    "# decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd106cd-015c-4947-99c3-e82e3e422b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(train_split*len(data))\n",
    "shakespear_train_data = shakespear_data[:n]\n",
    "shakespear_val_data = shakespear_data[n:]\n",
    "\n",
    "# # data loading\n",
    "# def get_batch(split):\n",
    "#     # generate a small batch of data of inputs x and targets y\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834b430f-8f3e-4546-9bdd-632df2c12c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding \n",
    "    PE(pos,2i) =sin(pos/10000^(2i/dmodel))\n",
    "    PE(pos,2i+1) =cos(pos/10000^(2i/dmodel))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        \"\"\"\n",
    "        pos: (seq_length, 1)\n",
    "        i: (1, d_model)\n",
    "        d_model: int (dimension of embedding)\n",
    "\n",
    "        return: (seq_length, d_model)\n",
    "        \"\"\"\n",
    "        power = 2*(i//2)/ torch.tensor(d_model, dtype=torch.float32)\n",
    "        return pos / (torch.pow(10000, power))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        assert len(inputs.shape) == 3\n",
    "        seq_length = inputs.shape[-2]\n",
    "        d_model = inputs.shape[-1]\n",
    "        angles = self.get_angles(\n",
    "            torch.arange(seq_length).unsqueeze(1),\n",
    "            torch.arange(d_model).unsqueeze(0),\n",
    "            d_model\n",
    "        )\n",
    "        \n",
    "        pe = torch.zeros(seq_length, d_model, device = inputs.device)\n",
    "        pe[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "        pe.unsqueeze(0)\n",
    "        return inputs + pe\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420298df-66ea-40d2-b286-384cc9db0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Positional encoding test\n",
    "\"\"\"\n",
    "test_pe_input = torch.tensor([\n",
    "    [[1,2,3], [2,3,4]], \n",
    "    [[3,4,5], [4,5,6]]\n",
    "]) #batch_size = 2, seq_length = 2, d_model = 3\n",
    "pos1_i0 = torch.sin(torch.tensor(1/math.pow(10000,0)))\n",
    "pos1_i1 = torch.cos(torch.tensor(1/math.pow(10000,0)))\n",
    "pos1_i2 = torch.sin(torch.tensor(1/math.pow(10000,2/float(3))))\n",
    "expected_pe = torch.tensor([\n",
    "    [[0, 1, 0], [pos1_i0, pos1_i1, pos1_i2]], \n",
    "    [[0, 1, 0], [pos1_i0, pos1_i1, pos1_i2]]\n",
    "]) + test_pe_input\n",
    "assert (expected_pe == PositionalEncoding().forward(test_pe_input)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "697f72bb-7913-4d70-9231-d01e9e9d2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "        self.query_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model).to(device)\n",
    "        self.key_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model).to(device)\n",
    "        self.value_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model).to(device)\n",
    "        self.final_lin = nn.Linear(in_features = self.d_model, out_features = self.d_model).to(device)\n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        softmax((QK.T)/sqrt(dk))V\n",
    "        \n",
    "        query: (batch_size, num_heads, seq_length, d_k)\n",
    "        key: (batch_size, num_heads, seq_length, d_k)\n",
    "        value: (batch_size, num_heads, seq_length, d_v)\n",
    "        mask: (batch_size, 1, 1, seq_length)\n",
    "        return: (batch_size, num_heads, seq_length, d_v)\n",
    "        \"\"\"\n",
    "        assert len(query.shape) == len(key.shape) and len(query.shape) == len(value.shape)\n",
    "        assert key.dtype == torch.float\n",
    "        \n",
    "        product = query @ (key.transpose(-1,-2))\n",
    "        \n",
    "        dk = torch.tensor(key.shape[-1], dtype = torch.float32)\n",
    "        sqrt_dk = torch.sqrt(dk)\n",
    "        scaled_product = product/sqrt_dk\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_product += mask * -1e9\n",
    "\n",
    "        softmax = torch.softmax(scaled_product, dim = -1)\n",
    "        attention = softmax @ value\n",
    "        return attention\n",
    "\n",
    "    def split_to_heads(self, inputs, batch_size):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_length, d_model)\n",
    "        return: (batch_size, n_proj, seq_length, d_model//n_heads)\n",
    "        \"\"\"\n",
    "        proj_inputs = inputs.view(batch_size, -1, self.n_heads, self.d_head)\n",
    "        return proj_inputs.transpose(1, 2)\n",
    "\n",
    "    def concat_from_heads(self, inputs, batch_size):\n",
    "        \"\"\"\n",
    "        input: (batch_size, n_proj, seq_length, d_model//n_heads)\n",
    "        return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        return inputs.transpose(2,1).reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query: (batch_size, seq_length, d_model)\n",
    "        key: (batch_size, seq_length, d_model)\n",
    "        value: (batch_size, seq_length, d_model)\n",
    "        mask: (batch_size, 1, 1, seq_length)\n",
    "        \n",
    "        return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        queries = self.query_lin(query.to(torch.float32))\n",
    "        keys = self.key_lin(key.to(torch.float32))\n",
    "        values = self.value_lin(value.to(torch.float32))\n",
    "\n",
    "        queries = self.split_to_heads(queries, batch_size)\n",
    "        keys = self.split_to_heads(keys, batch_size)\n",
    "        values = self.split_to_heads(values, batch_size)\n",
    "        \n",
    "        attention = self.scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        attention = self.concat_from_heads(attention, batch_size)\n",
    "        outputs = self.final_lin(attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5a7e40-bf19-436c-8002-aed556e2afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Head Attention Test\n",
    "\"\"\"\n",
    "def test_multi_head_attention_output_shape(input_shape = (32, 50, 64), n_heads = 8):\n",
    "     # (batch_size, seq_length, d_model)\n",
    "    mha = MultiHeadAttention(n_heads=n_heads, d_model=input_shape[-1])\n",
    "    batch_size, seq_length, d_model = input_shape\n",
    "    query = torch.rand(batch_size, seq_length, d_model)\n",
    "    key = torch.rand(batch_size, seq_length, d_model)\n",
    "    value = torch.rand(batch_size, seq_length, d_model)\n",
    "    outputs = mha.forward(query, key, value, mask)\n",
    "    assert outputs.shape == (batch_size, seq_length, d_model)\n",
    "    \n",
    "test_multi_head_attention_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3c07a71-1a5e-488c-a141-bc1422da6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scaled Dot Product Attention Test\n",
    "\"\"\"\n",
    "def test_scaled_dot_product():\n",
    "    test_scaled_dot_attention = torch.tensor([\n",
    "        [[1,2,3], [2,3,4]], \n",
    "        [[3,4,5], [4,5,6]]\n",
    "    ], dtype = torch.float32) #batch_size:2, seq_length: 2, d_model: 3\n",
    "    test_mask = torch.tensor([\n",
    "        [[0, 0]],\n",
    "        [[0, 1]]\n",
    "    ])# (batch_size, 1, seq_length)\n",
    "    test_product = torch.tensor([[[14., 20.],\n",
    "             [20., 29.]],\n",
    "            [[50., 62.],\n",
    "             [62., 77.]]], dtype = torch.float32)\n",
    "    test_scaled_product = test_product/math.sqrt(3)\n",
    "    test_scaled_product[1, :, 1] += -1e9 #applying mask\n",
    "    expected_attention = torch.softmax(test_scaled_product, dim = -1) @ test_scaled_dot_attention\n",
    "    assert (expected_attention == MultiHeadAttention(5, test_scaled_dot_attention.shape[-1]).scaled_dot_product_attention(test_scaled_dot_attention, test_scaled_dot_attention, test_scaled_dot_attention, test_mask)).all()\n",
    "\n",
    "test_scaled_dot_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d2dec44-e15c-4655-964e-8f301211e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super().__init__()\n",
    "        self.feed_forward_inner_lin = nn.Linear(in_features = d_model, out_features = dff).to(device)\n",
    "        self.feed_forward_relu = nn.ReLU()\n",
    "        self.feed_forward_outer_lin = nn.Linear(in_features = dff, out_features = d_model).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, seq_length, d_model)\n",
    "        return: (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        outputs = self.feed_forward_inner_lin(inputs)\n",
    "        outputs = self.feed_forward_relu(outputs)\n",
    "        outputs = self.feed_forward_outer_lin(outputs)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76cd4ef5-7b5e-4f20-a135-b67f90be3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feed Forward Test\n",
    "\"\"\"\n",
    "def test_feed_forward_output_shape(input_shape = (32, 50, 64), dff = 2048):\n",
    "     # (batch_size, seq_length, d_model)\n",
    "    ffn = FeedForward(input_shape[-1] ,dff)\n",
    "    batch_size, seq_length, d_model = input_shape\n",
    "    inputs = torch.rand(batch_size, seq_length, d_model)\n",
    "    outputs = ffn.forward(inputs)\n",
    "    assert outputs.shape == (batch_size, seq_length, d_model)\n",
    "    \n",
    "test_feed_forward_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0caf9ecc-42b5-4618-861f-fecf7c044f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dff, dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
    "        self.attention_dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "        self.feed_forward = FeedForward(d_model, dff)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "        self.feed_forward_dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                           inputs,\n",
    "                                           inputs,\n",
    "                                           mask)\n",
    "        attention = self.attention_dropout(attention)\n",
    "        attention = self.attention_layer_norm(inputs + attention)\n",
    "\n",
    "        outputs = self.feed_forward(attention)\n",
    "        outpus = self.feed_forward_dropout(outputs)\n",
    "        outputs = self.feed_forward_layer_norm(attention + outputs)\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0ceaa23-64c0-4860-8a97-5051852b5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder Layer Test\n",
    "\"\"\"\n",
    "def test_encoder_layer_output_shape(input_shape = (32, 50, 64), n_heads = 8, dff = 2048):\n",
    "     # (batch_size, seq_length, d_model)\n",
    "    encoder_layer = EncoderLayer(d_model = input_shape[-1] , n_heads = n_heads, dff = dff)\n",
    "    batch_size, seq_length, d_model = input_shape\n",
    "    inputs = torch.rand(batch_size, seq_length, d_model)\n",
    "    mask = torch.ones(batch_size, 1, 1, seq_length)\n",
    "    outputs = encoder_layer.forward(inputs, mask)\n",
    "    assert outputs.shape == (batch_size, seq_length, d_model)\n",
    "    \n",
    "test_encoder_layer_output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6257d3d-9088-415c-bcfc-1645839bfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_heads, \n",
    "                 dff = 2048,\n",
    "                 n_layers = 6, \n",
    "                 dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.encoder_layers = [EncoderLayer(d_model, n_heads, dff, dropout_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        outputs = self.encoder_layers[0](inputs, mask)\n",
    "        for i in range(1, self.n_layers):\n",
    "            outputs = self.encoder_layers[i](outputs, mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "373981a3-3ec7-483c-9975-2ecde1c7a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dff, dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.multi_head_masked_attention = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
    "        self.masked_attention_dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.masked_attention_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads=n_heads, d_model=d_model)\n",
    "        self.attention_dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.attention_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model, dff)\n",
    "        self.feed_forward_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "        self.feed_forward_dropout = nn.Dropout(p = dropout_rate)\n",
    "\n",
    "    def forward(self, inputs, enc_outputs, lookahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, generated_seq_length, d_model)\n",
    "        enc_outputs: (batch_size, seq_length, d_model)\n",
    "        lookahead_mask: (batch_size, 1, 1, seq_length)\n",
    "        padding_mask: (batch_size, 1, 1, seq_length)\n",
    "        \n",
    "        \"\"\"\n",
    "        masked_attention = self.multi_head_masked_attention(inputs,\n",
    "                                           inputs,\n",
    "                                           inputs,\n",
    "                                           lookahead_mask) # (batch_size, generated_seq_length, d_model)\n",
    "        masked_attention = self.masked_attention_dropout(masked_attention) # (batch_size, generated_seq_length, d_model)\n",
    "        masked_attention = self.masked_attention_layer_norm(inputs + masked_attention) # (batch_size, generated_seq_length, d_model)\n",
    "        \n",
    "        attention = self.multi_head_attention(masked_attention,\n",
    "                                           enc_outputs,\n",
    "                                           enc_outputs,\n",
    "                                           padding_mask)\n",
    "\n",
    "        attention = self.attention_dropout(attention)\n",
    "        attention = self.attention_layer_norm(masked_attention + attention)\n",
    "\n",
    "        outputs = self.feed_forward(attention)\n",
    "        outpus = self.feed_forward_dropout(outputs)\n",
    "        outputs = self.feed_forward_layer_norm(attention + outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1f8d1cc5-431a-4484-a53c-0bd414c5b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 n_heads, \n",
    "                 dff = 2048,\n",
    "                 n_layers = 6, \n",
    "                 dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.decoder_layers = [DecoderLayer(d_model, n_heads, dff, dropout_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def forward(self, inputs, enc_outputs, lookahead_mask, padding_mask):\n",
    "        outputs = self.decoder_layers[0](inputs, enc_outputs, lookahead_mask, padding_mask)\n",
    "        for i in range(1, self.n_layers):\n",
    "            outputs = self.decoder_layers[i](outputs, enc_outputs, lookahead_mask, padding_mask)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "96b84656-a44c-4427-9311-f223ef789d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                vocab_size,\n",
    "                d_model = 512,\n",
    "                n_heads = 8,\n",
    "                dff = 2048,\n",
    "                n_layers = 6,\n",
    "                dropout_rate = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.input_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "        self.encoder = Encoder(d_model, n_heads, dff, n_layers, dropout_rate)\n",
    "        self.decoder = Decoder(d_model, n_heads, dff, n_layers, dropout_rate)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size, bias = False).to(device)\n",
    "        \n",
    "        # Share the weights between the embedding and the output linear layer\n",
    "        self.fc_out.weight = torch.nn.Parameter(self.embedding.weight)\n",
    "        self.final_softmax = nn.Softmax(-1)\n",
    "\n",
    "    def create_padding_mask(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, seq_length)\n",
    "        return: (batch_size, 1, 1, seq_length)\n",
    "        \"\"\"\n",
    "        mask = (inputs == 0).unsqueeze(1).unsqueeze(2).to(torch.float32).to(device)\n",
    "        return mask\n",
    "\n",
    "    def create_lookahead_mask(self, inputs):\n",
    "        seq_len = inputs.size(1)\n",
    "        look_ahead_mask = 1 - torch.triu(torch.ones((seq_len, seq_len)), diagonal=1)\n",
    "        return look_ahead_mask.to(device)\n",
    "\n",
    "    def forward(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, seq_length)\n",
    "        outputs: (batch_size, generated_seq_length)\n",
    "        \"\"\"\n",
    "        embedded_inputs = self.embedding(inputs) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)) # (batch_size, seq_length, d_model)\n",
    "        pe_inputs = self.positional_encoding(embedded_inputs) # (batch_size, seq_length, d_model)\n",
    "        pe_inputs = self.input_dropout(pe_inputs)# (batch_size, seq_length, d_model)\n",
    "        \n",
    "        embedded_outputs = self.embedding(outputs) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))# (batch_size, generated_seq_length, d_model)\n",
    "        pe_outputs = self.positional_encoding(embedded_outputs) # (batch_size, generated_seq_length, d_model)\n",
    "        pe_outputs = self.input_dropout(pe_outputs)  # (batch_size, generated_seq_length, d_model)\n",
    "\n",
    "        padding_mask_encoder = self.create_padding_mask(inputs) #(batch_size, 1, 1, seq_length)\n",
    "        encoded_outputs = self.encoder(pe_inputs, padding_mask_encoder)# (batch_size, seq_length, d_model)\n",
    "        \n",
    "        padding_mask_decoder = self.create_padding_mask(inputs) # (batch_size, 1, 1, seq_length)\n",
    "        lookahead_mask_decoder = torch.maximum(\n",
    "            self.create_padding_mask(outputs), # (batch_size, 1, 1, generated_seq_length)\n",
    "            self.create_lookahead_mask(outputs) # (generated_seq_length, generated_seq_length)\n",
    "        )\n",
    "\n",
    "        decoded_outputs = self.decoder(pe_outputs,\n",
    "                                      encoded_outputs,\n",
    "                                      lookahead_mask_decoder,\n",
    "                                      padding_mask_decoder) #(batch_size, generated_seq_length, d_model)\n",
    "\n",
    "        outputs = self.fc_out(decoded_outputs) #(batch_size, generated_seq_length, vocab_size)\n",
    "        outputs = self.final_softmax(outputs) #(batch_size, generated_seq_length, vocab_size))\n",
    "        return outputs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07c6e0c0-54ab-4855-8b42-264572193e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRScheduler(LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, last_epoch=0, initial_lr = 1.7e-07):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['initial_lr'] = param_group['lr']\n",
    "        super(CustomLRScheduler, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    \n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        lr = (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "        \n",
    "        return [lr for _ in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2bf3858-6519-46b1-9161-cab794cf73be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.746928107421711e-07"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testing():\n",
    "    step = 1\n",
    "    warmup_steps = 4000\n",
    "    d_model = 512\n",
    "    arg1 = step ** -0.5\n",
    "    arg2 = step * (warmup_steps ** -1.5)\n",
    "    lr = (d_model ** -0.5) * min(arg1, arg2)\n",
    "    return lr\n",
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50abd0d2-fe83-4eed-9b11-26558bd4b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, files):\n",
    "        if files is not None:\n",
    "            self.tokenizer = ByteLevelBPETokenizer()\n",
    "            self.tokenizer.train(files=files, \n",
    "                                 vocab_size=VOCAB_SIZE, \n",
    "                                 min_frequency=2, \n",
    "                                 special_tokens=[\n",
    "                                     \"<PAD>\",\n",
    "                                     \"<START>\",\n",
    "                                     \"<END>\",\n",
    "                                     \"<UNK>\",\n",
    "                                 ])\n",
    "            self.tokenizer.pad_token = \"<PAD>\"\n",
    "            self.tokenizer.pad_token_id = 0\n",
    "            self.tokenizer.post_processor = TemplateProcessing(\n",
    "                                            single=\"<START> $A <END>\",\n",
    "                                            pair=\"<START> $A <END> $B:1 <END>:1\",\n",
    "                                            special_tokens=[(\"<PAD\", 0), (\"<START>\", 1), (\"<END>\", 2)],\n",
    "                                        )\n",
    "        else:\n",
    "            self.tokenizer = ByteLevelBPETokenizer(path_to_tokenizer + 'vocab.json',\n",
    "                                                  path_to_tokenizer + 'merges.txt')\n",
    "            self.tokenizer.pad_token = \"<PAD>\"\n",
    "            self.tokenizer.pad_token_id = 0\n",
    "            self.tokenizer.post_processor = TemplateProcessing(\n",
    "                                            single=\"<START> $A <END>\",\n",
    "                                            pair=\"<START> $A <END> $B:1 <END>:1\",\n",
    "                                            special_tokens=[(\"<PAD\", 0), (\"<START>\", 1), (\"<END>\", 2)],\n",
    "                                        )\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, encoded_text):\n",
    "        return self.tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d7b5c7d-c4de-4c7d-b6f9-71fcb3fab0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tknzr = Tokenizer(path_to_data + train_data_file)\n",
    "# tknzr.tokenizer.save_model(path_to_tokenizer)\n",
    "# tknzr.encode(\"<start>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "222817f7-881a-46dd-a621-bdfc61b5b86b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 87, 395, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(None)\n",
    "tokenizer.encode(\"test\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed1e4b9-8235-4e32-b092-07054326e459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9331\n",
      "Su-jeong (7) O\" (8) O&A (9) O&A Army (10) O&C Railroad (11) O&G (12) O&K (13) O&M (14) O&M Hausser (15) O&O (16) O&O Defrag (17) O&W (18) O&Y (19) O&YH Union (20) O&Y Hope Union (21) O&g (22) O' (23) O'Ahu 'Alauahio (24) O'Ahu 'Amakihi (25) O'B (26) O'Bama (27) O'Bananon Publishing (28) O'Banion Middle School (29) O'Bannon (30) O'Bannon, Kentucky (31) O'Bannon, Louisville (32) O'Bannon (DD-987) (33) O'Bannon (DD 987) (34) O'Bannon (surname) (35) O'Bannon Mill (36) O'Bannon Publishing (37) O'Bannon Woods State Park (38) O'Berry Neuro-Medical Center (39) O'Biden (40) O'Bleness Memorial Hospital (41) O'Boylan (42) O'Boyle (43) O'Boyle Donegal (44) O'Braein, Tighernach (45) O'Braien (46) O'Brian (47) O'Brian White (48) O'Brian Woodbine (49) O'Brien (50) O'Brien's-Bridge (51) O'Brien's Bridge (52) O'Brien's Tower (53) O'Brien, Argentina (54) O'Brien, George Donoghue (55) O'Brien, John (56) O'Brien, Michael (57) O'Brien, OR (58) O'Brien, Or (59) O'Brien, Oregon (60) O'Brien, Sean (61) O'Brien, TX (62) O'Brien, Terence Albert (63) O'Brien, Texas (64) O'Brien, Washington (65) O'Brien (1984) (66) O'Brien (Nineteen Eighty-Four) (67) O'Brien (Star Trek) (68) O'Brien (book character) (69) O'Brien (disambiguation) (70) O'Brien Award (71) O'Brien Baronets (72) O'Brien Bay (73) O'Brien Clan (74) O'Brien Cogeneration Inc. (75) O'Brien County (76) O'Brien County, IA (77) O'Brien County, Iowa (78) O'Brien Cup (79) O'Brien Field (80) O'Brien General Store and Post Office (81) O'Brien Institute (82) O'Brien Island (83) O'Brien Peak (84) O'Brien Press (85) O'Brien Provincial Park (86) O'Brien Smith (87) O'Brien Stadium (88) O'Brien Theatre (Arnprior) (89) O'Brien Township, Beltrami County, Minnesota (90) O'Brien Township, MN (91) O'Brien Township, Minnesota (92) O'Brien Trophy (93) O'Brien class destroyer (94) O'Brien class submarine (1972) (95) O'Brien granuloma (96) O'Brien test (97) O'Brienite Nationalist (98) O'Briens (99) O'Briens Bridge (100) O'Briens Creek (101) O'Briens Irish Sandwich Bars (102) O'Briens Tower (103) O'Briensbridge (104) O'Briensbridge-Montpelier (105) O'Brion (106) O'Brion, West Virginia (107) O'Bruadair, David (108) O'Bryan (109) O'Bryant High School (110) O'Bryant School (111) O'Bryant Square (112) O'Bryonville, Cincinnati, Ohio (113) O'Bryonville, Ohio (114) O'Byrne Cup (115) O'Byrne Cup 2007 (116) O'Byrne Cup 2008 (117) O'Byrne Cup 2009 (118) O'Byrne Shield (119) O'CHE (120) O'CHE1867 (121) O'CHE 1867 (122) O'Cahan (123) O'Cain Point (124) O'Callaghan (125) O'Callaghan's-Mills (126) O'Callaghans-Mills (127) O'Callaghans mills (128) O'Caml (129) O'Caml (programming language) (130) O'Caml programming language (131) O'Carolan (132) O'Carolan, Torlogh (133) O'Carroll (134) O'Casey (135) O'Casey (surname) (136) O'Cathasaigh (137) O'Cayz Corral (138) O'Ceann (139) O'Charley's (140) O'Charley's Inc. (141) O'Charleys (142) O'Che (143) O'Che1867 (144) O'Che 1867 (145) O'Chi Brown (146) O'Chiese First Nation (147) O'Collins (148) O'Connell (149) O'Connell's Spiny-rat (150) O'Connell's Spiny Rat (151) O'Connell, NSW, Australia (152) O'Connell, New South Wales (153) O'Connell, Ontario (154) O'Connell & Goldberg Creative Public Relation (155) O'Connell (footballer) (156) O'Connell (name) (157) O'Connell Baronets (158) O'Connell Bridge (159) O'Connell Center (160) O'Connell College Preparatory School (161) O'Connell College Prepartory School (162) O'Connell Consolidated High School (163) O'Connell High School (164) O'Connell High School (disambiguation) (165) O'Connell Nunatak (166) O'Connell Road (167) O'Connell School (168) O'Connell Schools (169) O'Connell Sports Center (170) O'Connell St (171) O'Connell St. (172) O'Connell Street (173) O'Connell Street, Limerick (174) O'Connell Street (disambiguation) (175) O'Connell of Derrynane (176) O'Connell street (177) O'Connells of Derrynane (178) O'Connells schools (179) O'Conner (180) O'Connor (181) O'Connor's Gunners (182) O'Connor's Landing (183) O'Connor, Australian Capital Territory (184) O'Connor, Canberra (185) O'Connor, Flannery (186) O'Connor, Jack (187) O'Connor, Ontario (188) O'Connor, Sandra Day (189) O'Connor, Western Australia (190) O'Connor-Parkview (191) O'Connor - Keogh official secrets trial (192) O'Connor Airlines (193) O'Connor Catholic High School (194) O'Connor Co-operative School (195) O'Connor High School (196) O'Connor Hospital (197) O'Connor Island (198) O'Connor Ministry (199) O'Connor Nunataks (200) O'Connor Park (201) O'Connor Peak (202) O'Connor Street (203) O'Connor Street (Ottawa) (204) O'Connor Township, Ontario (205) O'Connor v. Donaldson (206) O'Connors Rock (207) O'Connorville (208) O'Conor, Charles (209) O'Conor Don (210) O'Conor Don, Prince of Connacht (211) O'Cullenan, Gelasius (212) O'DWEEDS (213) O'Daly, Daniel (214) O'Daly, Donogh Mor (215) O'Daly, Donogh Mór (216) O'Davoren (217) O'Day (218) O'Day (crater) (219) O'Day (sailboat) (220) O'Day Mariner (221) O'Dea (222) O'Dea Castle (223) O'Dea High School (224) O'Dea House (Berwyn Heights, Maryland) (225) O'Dean (226) O'Death (227) O'Dell (228) O'Dell, British Columbia (229) O'Dempseys (230) O'Devany, Cornelius (231) O'Dogherty (232) O'Doherty (233) O'Doherty (surname) (234) O'Donnabhain v. Commissioner (235) O'Donnell (236) O'Donnell's Salamander (237) O'Donnell's salamander (238) O'Donnell, Edmund (239) O'Donnell, TX (240) O'Donnell, Texas (241) O'Donnell & Associates (242) O'Donnell & Tuomey (243) O'Donnell (TX) (244) O'Donnell Clan (245) O'Donnell Hall (246) O'Donnell Heights, Baltimore (247) O'Donnell ISD (248) O'Donnell Independent School District (249) O'Donnell Middle School (250) O'Donnell Park (251) O'Donnell Peak (252) O'Donnell of Tyrconnell (253) O'Donnell v Shanahan (254) O'Donnells, Newfoundland and Labrador (255) O'Donnelly (256) O'Donoghue (257) O'Donoghue's Opera (258) O'Donoghue's Pub (259) O'Donoghue of the Glens (260) O'Donohue (261) O'Donohue v. Canada (262) O'Donohue v. Canada, 2003 (263) O'Donohue v. Her Majesty The Queen (264) O'Donohue v. Her Majesty the Queen (265) O'Donohue v. Her Majesty the Queen, 2003 (266) O'Donohue v. Regina, 2003 (267) O'Donohue v. The Queen, 2003 (268) O'Donovan (269) O'Donovan, John (270) O'Donovan Rossa (271) O'Donovan Rossa (Skibbereen) (272) O'Donovan Rossa (football club) (273) O'Donovan Rossa Bridge (274) O'Donovan Rossa Magherafelt (275) O'Doul's (276) O'Douls (277) O'Dowd (278) O'Doyle Rules (279) O'Driscoll (280) O'Duffy (281) O'Duffy Cup (282) O'Dugan, John (283) O'Dweeds (284) O'Dwyer (285) O'Dwyer, Joseph (286) O'Dwyer (surname) (287) O'Dwyer VLe (288) O'Dwyers GAA (289) O'Dwyers of Kilnamanagh (290) O'Fallon (291) O'Fallon, IL (292) O'Fallon, Il (293) O'Fallon, Illinois (294) O'Fallon, MO (295) O'Fallon, Missouri (296) O'Fallon, Saint Louis (297) O'Fallon, St. Louis (298) O'Fallon, St Louis (299) O'Fallon (MO) (300) O'Fallon Township, St. Clair County, Illinois (301) O'Fallon Township High School (302) O'Farrell (303) O'Farrell Community School (304) O'Farrell Theatre (305) O'Farrell family (306) O'Flahertie (307) O'Flaherty (308) O'Flanagan (309) O'Friel (310) O'Gallachoir (311) O'Gallchobhair (312) O'Gara, Hess and Eisenhart (313) O'Gara-Hess & Eisenhardt (314) O'Gorman (315) O'Gorman Catholic High School (Sioux Falls, S (316) O'Gorman Catholic High School (Sioux Falls, S (317) O'Gorman High School (318) O'Gorman High School (Timmins, Ontario) (319) O'Gorman Mahon (320) O'Grady (321) O'Grady's Pub (322) O'Grady (surname) (323) O'Grady Lefroy (324) O'Grady v. Sparling (325) O'Griffey (326) O'Griffin (327) O'Grimacey (328) O'Hagan (329) O'Hagan, John (330) O'Hagan, Thomas (331) O'Hair (332) O'Halloran (333) O'Halloran Hill (334) O'Halloran Hill, South Australia (335) O'Hanlon (336) O'Hanlon, John (337) O'Hanlon Sept (338) O'Hanlon v Revenue and Customs Commissioners (339) O'Hara (340) O'Hara, Kieron (341) O'Hara, Theodore (342) O'Hara, U.S. Treasury (343) O'Hara, U. S. Treasury (344) O'Hara, US Treasury (345) O'Hara, U S Treasury (346) O'Hara, United States Treasury (347) O'Hara, Valentine (348) O'Hara Glacier (349) O'Hara Township (350) O'Hara Township, Allegheny County, PA (351) O'Hara Township, Allegheny County, Pennsylvan (352) O'Hara Township, PA (353) O'Hara Township, Pennsylvania (354) O'Hara Township (PA) (355) O'Hara Waltham Dial Company (356) O'Haran (357) O'Hare (358) O'Hare, Chicago (359) O'Hare, Illinois (360) O'Hare (Airport) (361) O'Hare (CTA) (362) O'Hare (airport) (363) O'Hare (disambiguation) (364) O'Hare (surname) (365) O'Hare Airport (366) O'Hare Airport Transit System (367) O'Hare IAP ARS (368) O'Hare International (369) O'Hare International Airport (370) O'Hare Modernization Program (371) O'Hare Ring Road (372) O'Hare Transfer (Metra) (373) O'Hare airport (374) O'Harra (375) O'Haskell (376) O'Haskell (programming language) (377) O'Haskell programming language (378) O'Haver (379) O'Hay (380) O'Hely, Patrick (381) O'Henry (382) O'Herlahy, Thomas (383) O'Higgins (384) O'Higgins, Ambrose and Bernard (385) O'Higgins, Chile (386) O'Higgins/San Martin Lake (387) O'Higgins/San Martín Lake (388) O'Higgins (Chilean frigate) (389) O'Higgins (frigate) (390) O'Higgins (region) (391) O'Higgins Department (392) O'Higgins Glacier (393) O'Higgins Lake (394) O'Higgins Park (395) O'Higgins Region (396) O'Higgins Region of Chile (397) O'Horan (398) O'Horten (399) O'Hurley (400) O'Hurley, Dermond (401) O'Hussey, Maelbrighte (402) O'Jays (403) O'Kanata (404) O'Kane (DDG 77) (405) O'Kane Building (406) O'Kanes (407) O'Kean (408) O'Kean, AR (409) O'Kean, Arkansas (410) O'Keefe (411) O'Keefe (Danny O'Keefe album) (412) O'Keefe Centre (413) O'Keefe Centre for the Performing Arts (414) O'Keefe Cup (415) O'Keefe Rail Trail (416) O'Keeffe (417) O'Kelly (418) O'Kelly-Riddick Stadium (419) O'Kelly Isley (420) O'Kelly Isley, Jr. (421) O'Kelly v. Trusthouse Forte plc (422) O'Kelly v Trusthouse Forte plc (423) O'LRY (424) O'LRY? (425) O'Lawlor (426) O'Leary (427) O'Leary, Arthur (428) O'Leary, John (429) O'Leary, Prince Edward Island (430) O'Leary, Sean (431) O'Leary, William (432) O'Leary-Inverness (433) O'Leary Island (434) O'Leary Manager 2000 (435) O'Leary Peak (436) O'Leno State Park (437) O'Level (438) O'Loan (439) O'Loghlen, Michael (440) O'Loghlen Baronets (441) O'Lone v. Estate of Shabazz (442) O'Loughlin (443) O'Loughlin Gaels (444) O'Loughlin Gaels GAA (445) O'Love Jacobsen (446) O'MAOILMHICHIL (447) O'Mahony (448) O'Malley (449) O'Malley, Australian Capital Territory (450) O'Malley, Canberra (451) O'Malley (surname) (452) O'Malley v. Simpson-Sears (453) O'MaoilMhichil (454) O'Maoilmhichil (455) O'Mara (456) O'Meara (457) O'Mega (458) O'Melveny & Meyers (459) O'Melveny & Myers (460) O'Melveny & Myers, LLP (461) O'Melveny & Myers LLC (462) O'Melveny & Myers LLP (463) O'Melveny Park (464) O'Mera Lake (465) O'Moore Creagh (466) O'Moore Creagh (disambiguation) (467) O'Moore Medal (468) O'Moore Park (469) O'More College of Design (470) O'NO 99 (471) O'Nan group (472) O'Neal (473) O'Neal, Edward Asbury (474) O'Neal, Shaquille (475) O'Neal, Virginia (476) O'Neal (name) (477) O'Neal Airport (478) O'Neal Compton (479) O'Neal Island (480) O'Neal Steel (481) O'Neal Steel, Inc. (482) O'Neals (483) O'Neals, California (484) O'Neil (485) O'Neil's (486) O'Neil's (department store) (487) O'Neil, Florida (488) O'Neil \"Give 'em Hell\" Bell (489) O'Neil Bell (490) O'Neil De Noux (491) O'Neil Ford (492) O'Neil Highway (493) O'Neil Longson (494) O'Neil Place (495) O'Neil Place, California (496) O'Neil Richards (497) O'Neil Thompson (498) O'Neil Wilson (499) O'Neil and Company Incline (500) O'Neilism (501) O'Neill (502) O'Neill's (503) O'Neill's (department store) (504) O'Neill's Adventure Land (505) O'Neill's Adventureland (506) O'Neill's formula (507) O'Neill's of Puerto Rico / O'Neill's of the F (508) O'Neill, California (509) O'Neill, Eugene Gladstone (510) O'Neill, Jack (511) O'Neill, Kevin (512) O'Neill, NE (513) O'Neill, Nebraska (514) O'Neill, Paul (515) O'Neill, Robert (516) O'Neill-class Battleship (517) O'Neill (brand) (518) O'Neill (surname) (519) O'Neill Baronets (520) O'Neill Bell (521) O'Neill Building (522) O'Neill Center (523) O'Neill Clan (524) O'Neill Collegiate and Vocational Institute (525) O'Neill Cylinder (526) O'Neill Dam (527) O'Neill Donaldson (528) O'Neill Dynasty (529) O'Neill Family Hall (530) O'Neill Forebay (531) O'Neill Forebay Dam (532) O'Neill Hall (533) O'Neill Hall (University of Notre Dame) (534) O'Neill History (535) O'Neill House Office Building (536) O'Neill Park (537) O'Neill Sea Odyssey (538) O'Neill Sebastian Inlet Pro (539) O'Neill Spencer (540) O'Neill Tunnel (541) O'Neill Wetsuit (542) O'Neill World Cup of Surfing (543) O'Neill class battleship (544) O'Neill cylinder (545) O'Neill family (546) O'Neill formula (547) O'Neill habitat (548) O'Neill ministry (549) O'Neill of Clannaboy (550) O'Neill v. Phillips (551) O'Neill v Phillips (552) O'Neill wetsuit (553) O'Neills (554) O'Neills of Puerto Rico (555) O'Neills of the Fews (556) O'Niell Tunnel (557) O'Nyong-Nyong virus (558) O'Nyong-nyong fever (559) O'Odham language (560) O'Parvardigar (561) O'Queely, Malachias (562) O'Quin (563) O'Quinn (564) O'Quinn, Texas (565) O'Quinn Law Library (566) O'Quinn Medical Tower at St. Luke's Hospital (567) O'Rahilly's Historical Model (568) O'Rahilly's historical model (569) O'Ratz (570) O'Regan (571) O'Regan's, Newfoundland and Labrador (572) O'Reilly (573) O'Reilly's Bridge, Ontario (574) O'Reilly, Bill (575) O'Reilly, Edmund (576) O'Reilly, Hugh (577) O'Reilly, Myles William Patrick (578) O'Reilly & Associates (579) O'Reilly (disambiguation) (580) O'Reilly 200 (581) O'Reilly 200 presented by Valvoline (582) O'Reilly 200 presented by Valvoline Maxlife (583) O'Reilly 300 (584) O'Reilly 400K (585) O'Reilly Associates (586) O'Reilly Auto Parts (587) O'Reilly Auto Parts 250 (588) O'Reilly Auto Parts Puerto Rico Tip-off (589) O'Reilly Automotive (590) O'Reilly Automotive Inc. (591) O'Reilly Books (592) O'Reilly Challenge (593) O'Reilly Emerging Technology Conference (594) O'Reilly Factor (595) O'Reilly Factor for Kids (596) O'Reilly Foundation (597) O'Reilly House (598) O'Reilly Labs Code Search (599) O'Reilly Media (600) O'Reilly Media, Inc. (601) O'Reilly Open Source Convention (602) O'Reilly Press (603) O'Reilly Raceway Park (604) O'Reilly Raceway Park at Indianapolis (605) O'Reilly Theater (606) O'Reilly Theatre (607) O'Reilly and Associates (608) O'Reilly and the Age of Persuasion (609) O'Reilly on Advertising (610) O'Reilly v. Morse (611) O'Reily Factor (612) O'Riely Factor (613) O'Riordan (614) O'Rorke (615) O'Rorke, Patrick Henry (616) O'Rothlain (617) O'Rourke (618) O'Rourke's Diner (619) O'Rourke (1591) (620) O'Rourke–McFadden Trophy (621) O'Ryan (622) O'Ryan (album) (623) O'Sailor (624) O'Scannlain (625) O'Shane (626) O'Shaughnessy (627) O'Shaughnessy's Boy (628) O'Shaughnessy Dam (629) O'Shaughnessy Dam (Ohio) (630) O'Shaughnessy Dam controversies (631) O'Shaughnessy Hall (632) O'Shaughnessy Hollow (633) O'Shea (634) O'Shea Jackson (635) O'Shea and Bennett's Siding, Walhalla line, V (636) O'Shea and Bennett's Siding railway station, (637) O'Shea and Whelan (638) O'Shea brothers (639) O'Shean's View (640) O'Shean's View, California (641) O'Sheas (642) O'Sheas Casino (643) O'Sheas Casino Las Vegas (644) O'Siochfhradha (645) O'Stravaganza - Vivaldi in Ireland (646) O'Sullivan (647) O'Sullivan's First Law (648) O'Sullivan's Law (649) O'Sullivan, John (650) O'Sullivan-McLeod syndrome (651) O'Sullivan Army Heliport (652) O'Sullivan Beach, South Australia (653) O'Sullivan Beare, Philip (654) O'Sullivan College of Montreal (655) O'Sullivan Dam (656) O'Sullivan v Noarlunga Meat Ltd (657) O'Sullivan v Noarlunga Meat Ltd (No 2) (658) O'Toole (659) O'Toole's (660) O'Toole's Corollary (661) O'Toole, Saint Lawrence (662) O'Toole (family) (663) O'Tooles GAC (664) O'Tuathail (665) O'Zone (666) O'Zorgnax's Pub (667) O' (digraph) (668) O' Brother, Where Art Thou (669) O' Brother, Where Art Thou?\n"
     ]
    }
   ],
   "source": [
    "def find_max_seq_length():\n",
    "    max_length = 0\n",
    "    sentence = \"\"\n",
    "    tokenizer = Tokenizer(None)\n",
    "    for i in english_train + english_test + english_validation + german_train + german_test + german_validation:\n",
    "        encoded = tokenizer.encode(i)\n",
    "        if max_length < len(encoded.ids):\n",
    "            max_length = len(encoded.ids)\n",
    "            sentence = i\n",
    "    print(max_length) # 9331\n",
    "    print(sentence)\n",
    "# find_max_seq_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ddf597-10d4-4973-b389-e6c7fe1c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_seq_length_for_batch(data):\n",
    "    max_length = 0\n",
    "    for encoded in data:\n",
    "        if max_length < len(encoded.ids):\n",
    "            max_length = len(encoded.ids)\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "31bb0afb-bfb8-4e6a-8c1a-2ba387af4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def get_batch(split, tokenizer, batch_size = batch_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    x = [tokenizer.encode(i).ids for i in data.values[ix, 1]]\n",
    "    y = [tokenizer.encode(i).ids for i in data.values[ix, 0]]\n",
    "    x = pad_sequence([torch.tensor(i) for i in x], batch_first= True, padding_value=0)\n",
    "    y = pad_sequence([torch.tensor(i) for i in y], batch_first= True, padding_value=0)\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def transformer_loss(target, pred):\n",
    "    # Compute the cross-entropy loss\n",
    "    return criterion(pred.transpose(-1, -2), target)\n",
    "    \n",
    "\n",
    "def model_predict(X, target_max_length = 9400, sos_token_output = 1, eos_token_output =2):\n",
    "    X = X.unsqueeze(0)\n",
    "    out_sentence = torch.tensor(sos_token_output)\n",
    "    output = out_sentence.unsqueeze(axis=0).unsqueeze(0)\n",
    "    logits = model(X, output)\n",
    "    for _ in range(target_max_len):\n",
    "        predictions = model(inputs, output) \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        predicted_id = torch.tensor(torch.argmax(prediction, axis=-1), dtype = torch.int32)\n",
    "        if predicted_id == eos_token_output:\n",
    "            return torch.squeeze(output, axis=0)\n",
    "        # Concat the predicted word to the output sequence\n",
    "        output = torch.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "def model_test(X, Y):\n",
    "    outputs = Y[:,:-1]\n",
    "    X, outputs = X.to(device), outputs.to(device)\n",
    "    logits = model(X, outputs)\n",
    "    loss = transformer_loss(Y[:,1:].to(device), logits)\n",
    "    return loss\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss(tokenizer, sos_token_output = 1, eval_iters = 5):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # print(\"Evaluation iteration\", k)\n",
    "            X, Y = get_batch(split, tokenizer)\n",
    "            loss = model_test(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ae41f-eafc-4da7-bd6e-9d015e10c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "eval_interval = 10\n",
    "tokenizer = Tokenizer(None)\n",
    "model = Transformer(VOCAB_SIZE,\n",
    "                   D_MODEL,\n",
    "                    N_HEADS,\n",
    "                    DFF,\n",
    "                    N_LAYERS,\n",
    "                    DROPOUT_RATE)\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1.7e-07, betas=(0.9, 0.98), eps = 1e-9)\n",
    "scheduler = CustomLRScheduler(optimizer, d_model=512, warmup_steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd666ac-7cd8-425a-bfa1-ea5f9089c2a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7888 M parameters\n",
      "step 0: train loss 8.2160, val loss 8.2157\n",
      "step 10: train loss 8.2157, val loss 8.2159\n",
      "step 20: train loss 8.2162, val loss 8.2158\n",
      "step 30: train loss 8.2156, val loss 8.2149\n",
      "step 40: train loss 8.2153, val loss 8.2160\n",
      "step 50: train loss 8.2153, val loss 8.2147\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(tokenizer)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', tokenizer)\n",
    "    \n",
    "\n",
    "    # evaluate the loss\n",
    "    loss = model_test(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7628b6-edf2-4482-b4c9-0b35df7a5abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
