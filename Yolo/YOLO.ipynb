{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08777d1-da1b-4193-8f0b-b6cf5432b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1a1d5a-c7d6-4551-95c0-e2cff50cf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo(nn.Module):\n",
    "    def __init__(self, in_channels = 3, split_size = 7, n_bboxes = 2, n_classes = 20):\n",
    "        super().__init__()\n",
    "        self.conv_network = nn.Sequential(\n",
    "            self._conv_layer(in_channels, 64, 7, stride = 2, padding = 3),\n",
    "            self._get_max_pool(),\n",
    "            self._conv_layer(64, 192, 3, 1, 1),\n",
    "            self._get_max_pool(),\n",
    "            self._conv_layer(192, 128, 1, 1, 0),\n",
    "            self._conv_layer(128, 256, 3, 1, 1),\n",
    "            self._conv_layer(256, 256, 1, 1, 0),\n",
    "            self._conv_layer(256, 512, 3, 1, 1),\n",
    "            self._get_max_pool(),\n",
    "            self._get_four_conv_block_with_512_out(),\n",
    "            self._conv_layer(512, 512, 1, 1, 0),\n",
    "            self._conv_layer(512, 1024, 3, 1, 1),\n",
    "            self._get_max_pool(),\n",
    "            self._get_four_conv_block_with_1024_out(),\n",
    "            self._conv_layer(1024, 1024, 3, 1, 1),\n",
    "            self._conv_layer(1024, 1024, 3, stride = 2, padding =1),\n",
    "            self._conv_layer(1024, 1024, 3, 1, 1),\n",
    "            self._conv_layer(1024, 1024, 3, 1, 1),\n",
    "        )\n",
    "        self.fc_network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*split_size*split_size, 4096),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(4096, split_size * split_size * (n_bboxes * 5 + n_classes))\n",
    "        )\n",
    "\n",
    "    def _conv_layer(self,in_channels, out_channels, kernel_size, stride = 1, padding = 0):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels= out_channels, kernel_size = kernel_size, stride = stride, padding = padding),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "    \n",
    "    def _get_max_pool(self):\n",
    "        return nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "    def _get_four_conv_block_with_512_out(self):\n",
    "        return nn.Sequential(\n",
    "            self._get_one_and_three_conv_with_512_out(), \n",
    "            self._get_one_and_three_conv_with_512_out(),\n",
    "            self._get_one_and_three_conv_with_512_out(),\n",
    "            self._get_one_and_three_conv_with_512_out()\n",
    "        )\n",
    "\n",
    "    def _get_one_and_three_conv_with_512_out(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1, 1, 0), \n",
    "            nn.Conv2d(256, 512, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def _get_four_conv_block_with_1024_out(self):\n",
    "        return nn.Sequential(\n",
    "            self._get_one_and_three_with_1024_out(),\n",
    "            self._get_one_and_three_with_1024_out()\n",
    "        )\n",
    "        \n",
    "    def _get_one_and_three_with_1024_out(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, 1, 1, 0), \n",
    "            nn.Conv2d(512, 1024, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, image_len, image_width, channels)\n",
    "        returns: (batch_size, split_size, split_size, n_bboxes*5 + n_classes)\n",
    "        \"\"\"\n",
    "        conv_output = self.conv_network(inputs)\n",
    "        # conv_output = inputs\n",
    "        # for layer in self.conv_network:\n",
    "        #     conv_output = layer(conv_output)\n",
    "        #     print(f'Layer {layer}: output shape = {conv_output.shape}')\n",
    "        return self.fc_network(conv_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44223a3-348f-46ef-921f-56aaae50ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1e-3 + (1e-2 - 1e-3) * (epoch / 10)  # Linearly increase from 1e-3 to 1e-2\n",
    "    elif epoch < 85:\n",
    "        return 1e-2  # Keep constant at 1e-2\n",
    "    elif epoch < 115:\n",
    "        return 1e-3  # Decrease to 1e-3\n",
    "    else:\n",
    "        return 1e-4  # Decrease to 1e-4\n",
    "\n",
    "\n",
    "class CustomLRScheduler:\n",
    "    def __init__(self, optimizer, lr_func):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_func = lr_func\n",
    "\n",
    "    def step(self, epoch):\n",
    "        lr = self.lr_func(epoch)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fff0c9aa-317b-471c-aafa-b5fd581db8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2, 1])\n",
      "torch.Size([1, 2, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 2)\n",
    "b = torch.randn(1, 2, 2)\n",
    "torch.max(a.unsqueeze(-1),b.unsqueeze(-2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f9243a-a24a-440c-bbfb-2c0c8271bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(box1, box2):\n",
    "    \"\"\"\n",
    "    box1: (batch_size, split_size, split_size, n_boxes, 5)\n",
    "    box2: (batch_size, split_size, split_size, n_boxes, 5)\n",
    "\n",
    "    return: (batch_size, split_size, split_size, n_boxes, n_boxes) float representing iou\n",
    "    \"\"\"\n",
    "    # Unpack the coordinates and dimensions\n",
    "    x1_center, y1_center, w1, h1 = box1[..., 0], box1[..., 1], box1[..., 2], box1[..., 3]\n",
    "    x2_center, y2_center, w2, h2 = box2[..., 0], box2[..., 1], box2[..., 2], box2[..., 3]\n",
    "\n",
    "    x1_min, y1_min = x1_center - w1 / 2, y1_center - h1 / 2\n",
    "    x1_max, y1_max = x1_center + w1 / 2, y1_center + h1 / 2\n",
    "    x2_min, y2_min = x2_center - w2 / 2, y2_center - h2 / 2\n",
    "    x2_max, y2_max = x2_center + w2 / 2, y2_center + h2 / 2\n",
    "\n",
    "    inter_xmin = torch.max(x1_min.unsqueeze(-1), x2_min.unsqueeze(-2))\n",
    "    inter_ymin = torch.max(y1_min.unsqueeze(-1), y2_min.unsqueeze(-2))\n",
    "    inter_xmax = torch.min(x1_max.unsqueeze(-1), x2_max.unsqueeze(-2))\n",
    "    inter_ymax = torch.min(y1_max.unsqueeze(-1), y2_max.unsqueeze(-2))\n",
    "\n",
    "    inter_width = torch.clamp(inter_xmax - inter_xmin, min=0)\n",
    "    inter_height = torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    union_area = box1_area.unsqueeze(-1) + box2_area.unsqueeze(-2) - inter_area\n",
    "\n",
    "    iou = inter_area / torch.clamp(union_area, min=1e-8)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def non_max_suppression(boxes, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression (NMS) on the bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    boxes: shape (batch_size, 5)\n",
    "           Each row contains [x, y, w, h, score] coordinates of a bounding box.\n",
    "    iou_threshold: float\n",
    "                   IoU threshold for suppressing boxes.\n",
    "\n",
    "    Returns:\n",
    "    indices: list of int\n",
    "             Indices of the bounding boxes to keep.\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Convert boxes to numpy arrays (if not already)\n",
    "    boxes = torch.tensor(boxes)\n",
    "    scores = torch.tensor(scores)\n",
    "\n",
    "    # Compute the bottom-right coordinates and area of each box\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    w = boxes[:, 2]\n",
    "    h = boxes[:, 3]\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    areas = w * h\n",
    "\n",
    "    # Get the indices of the boxes sorted by scores in descending order\n",
    "    order = boxes[:, 4].argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        # The index of the current box with the highest score\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        # Compute the IoU of the kept box with the remaining boxes\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w_inter = np.maximum(0, xx2 - xx1)\n",
    "        h_inter = np.maximum(0, yy2 - yy1)\n",
    "        inter_area = w_inter * h_inter\n",
    "\n",
    "        iou = inter_area / (areas[i] + areas[order[1:]] - inter_area)\n",
    "\n",
    "        # Keep boxes with IoU less than the threshold\n",
    "        inds = np.where(iou <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6149af-b279-4369-a21b-c153b2018162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1923, -0.0761, -0.3437],\n",
      "         [ 1.3099,  0.0792,  1.6286]],\n",
      "\n",
      "        [[ 0.3423,  1.0475, -0.3291],\n",
      "         [ 0.5795,  0.5726, -1.1467]]])\n",
      "tensor([[[[0.2036, 0.1570, 0.2786],\n",
      "          [0.8855, 0.2941, 0.7706],\n",
      "          [0.6602, 0.2587, 0.5265]],\n",
      "\n",
      "         [[0.4236, 0.5903, 0.1919],\n",
      "          [0.4959, 0.5569, 0.1747],\n",
      "          [0.2349, 0.2248, 0.6572]]],\n",
      "\n",
      "\n",
      "        [[[0.8155, 0.0224, 0.3115],\n",
      "          [0.2994, 0.4425, 0.2509],\n",
      "          [0.6869, 0.0724, 0.8641]],\n",
      "\n",
      "         [[0.5276, 0.1692, 0.9147],\n",
      "          [0.5977, 0.8769, 0.9470],\n",
      "          [0.8026, 0.5486, 0.6304]]]])\n",
      "tensor([[[-0.1703, -0.0224, -0.2649],\n",
      "         [ 0.6496,  0.0441,  0.2845]],\n",
      "\n",
      "        [[ 0.1025,  0.4635, -0.0826],\n",
      "         [ 0.3464,  0.5022, -1.0859]]])\n",
      "tensor([[-0.0224,  0.6496],\n",
      "        [ 0.4635,  0.5022]]) tensor([[1, 0],\n",
      "        [1, 1]])\n",
      "torch.Size([2, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1923, -1.0000, -0.3437],\n",
       "         [-1.0000,  0.0792,  1.6286]],\n",
       "\n",
       "        [[ 0.3423, -1.0000, -0.3291],\n",
       "         [ 0.5795, -1.0000, -1.1467]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test =  torch.randn(2,2,3)\n",
    "iou_matrix = torch.rand(2,2,3,3) # batch, split, box, box\n",
    "\n",
    "values, indices = torch.max(iou_matrix[..., 1, :] * test, dim = -1)\n",
    "\n",
    "test.scatter_(-1, indices.unsqueeze(-1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd99fe5a-6429-4513-8660-055fb2d94e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, split_size = 7, n_bboxes = 2, n_classes = 20, lambda_coord = 5, lambda_noobj = 0.5):\n",
    "        super().__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.split_size = split_size\n",
    "        self.n_bboxes = n_bboxes\n",
    "        self.n_classes = n_classes\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def assign_boxes(self, pred_boxes, gt_boxes, batch_size):\n",
    "        gt_boxes_reshaped = gt_boxes.view(batch_size, self.split_size, self.split_size, self.n_bboxes, 5)\n",
    "        pred_boxes_reshaped = pred_boxes.view(batch_size, self.split_size, self.split_size, self.n_bboxes, 5)\n",
    "\n",
    "        iou_matrix = intersection_over_union(pred_boxes_reshaped, gt_boxes_reshaped)\n",
    "\n",
    "        gt_assigned = torch.full((batch_size, self.split_size, self.split_size, self.n_bboxes,), -1, dtype=torch.long)  # -1 means not assigned\n",
    "        pred_assigned = torch.full((batch_size, self.split_size, self.split_size, self.n_bboxes,), -1, dtype=torch.long)\n",
    "        \n",
    "        gt_available = torch.ones((batch_size, self.split_size, self.split_size, self.n_bboxes), dtype=torch.bool)\n",
    "        gt_available = gt_available.to(device)\n",
    "        unavailable_gt_box = (gt_boxes_reshaped[..., 4] == 0)\n",
    "        gt_available[unavailable_gt_box] = 0\n",
    "\n",
    "        for i in range(self.n_bboxes):\n",
    "            iou_max_values, gt_indices = torch.max(iou_matrix[..., i, :] * gt_available.float(), dim=-1)\n",
    "\n",
    "            pred_assigned[..., i] = gt_indices\n",
    "            \n",
    "            gt_available.scatter_(-1, gt_indices.unsqueeze(-1), -1)\n",
    "        \n",
    "            gt_assigned[..., gt_indices] = i\n",
    "\n",
    "        return gt_assigned, pred_assigned\n",
    "\n",
    "    def get_gt_boxes(self, target):\n",
    "        \"\"\"\n",
    "        target: (batch_size, split_size, split_size, n_bboxes, (4+1+n_classes))\n",
    "        \"\"\"\n",
    "        gt_boxes = target[..., :5]\n",
    "        gt_classes = target[..., 5:]\n",
    "        gt_classes = gt_classes.any(dim = 3).float()\n",
    "        return gt_boxes, gt_classes\n",
    "        \n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, split_size, split_size, n_bboxes*5 + n_classes)\n",
    "        target: (batch_size, split_size, split_size, n_bboxes, (4+1+n_classes)) the second last field in the target tells us if object exists or not\n",
    "        \"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        predictions = predictions.view(batch_size, self.split_size, self.split_size, self.n_bboxes*5 + self.n_classes)\n",
    "        \n",
    "        pred_boxes = predictions[..., :self.n_bboxes * 5].view(batch_size, self.split_size, self.split_size, self.n_bboxes, 5)\n",
    "        pred_classes = predictions[..., self.n_bboxes * 5:]\n",
    "\n",
    "        gt_boxes, gt_classes = self.get_gt_boxes(target)\n",
    "        if gt_boxes.size(0) == 0:\n",
    "            return torch.tensor(0.0)\n",
    "\n",
    "        gt_assigned, pred_assigned = self.assign_boxes(pred_boxes, gt_boxes, batch_size)\n",
    "\n",
    "        assigned_pred = pred_assigned != -1\n",
    "        unassigned_pred = pred_assigned == -1\n",
    "\n",
    "        pred_box_coords = pred_boxes[..., :4][assigned_pred]\n",
    "        pred_box_conf = pred_boxes[..., 4]\n",
    "\n",
    "        indices = torch.nonzero(assigned_pred, as_tuple=False)\n",
    "        if indices.shape[0] == 0:\n",
    "            print(\"gt assigned\", (gt_assigned != -1).any())\n",
    "            print(\"gt\", target[target[..., 4] == 1])\n",
    "            print(\"predictions nan\", torch.isnan(predictions).any())\n",
    "            print(\"gt nan\", torch.isnan(target).any())\n",
    "            print(\"pred\", pred_boxes[target[..., 4] == 1])\n",
    "            print(\"assigned pred\", assigned_pred.any())\n",
    "            print(\"pred_box\", pred_boxes.shape)\n",
    "            print(\"object exists\", (target[..., 4] == 1).any())\n",
    "        gt_box_coords = torch.stack([gt_boxes[batch, split1, split2, pred_assigned[batch, split1, split2, box],:-1] for batch, split1, split2, box in indices])\n",
    "        # gt_class_assigned = torch.stack([gt_classes[batch, split1, split2, pred_assigned[batch, split1, split2, box]] for batch, split1, split2, box in indices])\n",
    "        \n",
    "        # gt_box_coords = gt_boxes[..., :4][pred_assigned[assigned_pred]]\n",
    "\n",
    "        coord_loss = self.lambda_coord * (\n",
    "            self.mse_loss(pred_box_coords[..., :2], gt_box_coords[..., :2]) +\n",
    "            self.mse_loss(torch.sqrt(pred_box_coords[..., 2:]), torch.sqrt(gt_box_coords[..., 2:]))\n",
    "        )\n",
    "        object_loss = self.mse_loss(pred_box_conf[assigned_pred], \n",
    "                                    torch.ones_like(pred_box_conf[assigned_pred]))\n",
    "        \n",
    "        no_object_mask = (pred_assigned == -1).float()\n",
    "        no_object_loss = self.lambda_noobj * self.mse_loss(pred_box_conf[unassigned_pred], \n",
    "                                                           torch.zeros_like(pred_box_conf[unassigned_pred]))\n",
    "        class_loss = self.mse_loss(pred_classes, gt_classes)\n",
    "\n",
    "        total_loss = coord_loss + object_loss + no_object_loss + class_loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f080504f-f5a3-45be-9378-b7874dae1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0661, -0.9601,  0.2515,  2.4217],\n",
      "          [ 1.3241, -0.3079, -1.5554, -0.4353]],\n",
      "\n",
      "         [[ 0.8557,  0.1420, -0.9132, -0.9046],\n",
      "          [ 2.1552, -0.0372, -0.6626,  0.0683]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4454,  0.6649, -1.6457,  3.0212],\n",
      "          [-0.3633,  0.1150, -0.7448,  0.7338]],\n",
      "\n",
      "         [[ 0.3736, -1.9806, -0.5421, -0.2067],\n",
      "          [-0.2966, -0.3188, -0.8786, -0.5012]]]])\n",
      "tensor([[[0, 2],\n",
      "         [0, 0]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [0, 1]]])\n",
      "tensor([[[ True, False],\n",
      "         [ True,  True]],\n",
      "\n",
      "        [[ True, False],\n",
      "         [ True,  True]]])\n",
      "torch.Size([6])\n",
      "tensor([0, 0, 0, 0, 0, 1])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 1],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0],\n",
      "        [1, 1, 1]])\n",
      "torch.Size([6, 3])\n",
      "(tensor([0, 0, 0, 1, 1, 1]), tensor([0, 1, 1, 0, 1, 1]), tensor([0, 0, 1, 0, 0, 1]))\n",
      "torch.Size([6, 4])\n",
      "tensor([[ 0.0661, -0.9601,  0.2515,  2.4217],\n",
      "        [ 0.8557,  0.1420, -0.9132, -0.9046],\n",
      "        [ 0.8557,  0.1420, -0.9132, -0.9046],\n",
      "        [ 1.4454,  0.6649, -1.6457,  3.0212],\n",
      "        [ 0.3736, -1.9806, -0.5421, -0.2067],\n",
      "        [-0.2966, -0.3188, -0.8786, -0.5012]])\n"
     ]
    }
   ],
   "source": [
    "test_1 = torch.randn(2,2,2,4)\n",
    "test_2 =torch.randint(3,(2,2,2))\n",
    "cond = test_2 != 2\n",
    "indices = torch.nonzero(cond, as_tuple=False)\n",
    "values = torch.stack([test_1[i, j, test_2[i, j, k],:] for i, j, k in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a79a57-4355-4792-959c-aad45d4951f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = \"./VOCdevkit/VOC2012/Annotations/\"\n",
    "layout = \"./VOCdevkit/VOC2012/ImageSets/Main/\"\n",
    "images = \"./VOCdevkit/VOC2012/JPEGImages/\"\n",
    "def extract_filenames(train_file_path):\n",
    "    filenames = []\n",
    "    \n",
    "    with open(train_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if parts:\n",
    "                # The image ID is the first part of each line\n",
    "                filename = parts[0]\n",
    "                filenames.append(filename)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.find('filename').text\n",
    "    objects_and_bboxes = []\n",
    "\n",
    "    size = root.find('size')\n",
    "    img_width = float(size.find('width').text)   \n",
    "    img_height = float(size.find('height').text)\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        # Extract object class\n",
    "        obj_class = obj.find('name').text\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "        # Calculate x_center, y_center, width, height  \n",
    "        x_center = ((xmin + xmax) / 2) / img_width\n",
    "        y_center = ((ymin + ymax) / 2) / img_height\n",
    "        width = (xmax - xmin) / img_width\n",
    "        height = (ymax - ymin) / img_height\n",
    "\n",
    "        # Store object and bounding box information\n",
    "        objects_and_bboxes.append({\n",
    "            'class': obj_class,\n",
    "            'bbox': torch.tensor([x_center, y_center, width, height]),\n",
    "        })\n",
    "\n",
    "    return filename, objects_and_bboxes\n",
    "def load_image(image_file):\n",
    "    return Image.open(image_file)\n",
    "    \n",
    "train_files = extract_filenames(layout + \"train.txt\")\n",
    "val_files = extract_filenames(layout + \"val.txt\")\n",
    "\n",
    "classes = set()\n",
    "for filename in train_files:\n",
    "    _, objects = parse_xml(annotations + filename + '.xml')\n",
    "    classes.update([c['class'] for c in objects])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c428d05a-ce4f-4d27-a8a3-50874ec21093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "for filename in train_files:\n",
    "    _, objects = parse_xml(annotations + filename + '.xml')\n",
    "    image = load_image(images + filename +'.jpg')\n",
    "    image = image.convert(\"RGB\")\n",
    "    original_size = image.size\n",
    "    print(original_size[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f29e61-c31e-4dac-a967-a0a6d101391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data = train_files, split_size = 7, n_bboxes = 2, n_classes = 20, class_dictionary = None):\n",
    "        self.data = data\n",
    "        self.transformToTensor = transforms.ToTensor()\n",
    "        self.transformToResize = transforms.Resize((448,448))\n",
    "        self.split_size = split_size\n",
    "        self.n_bboxes = n_bboxes\n",
    "        self.n_classes = n_classes\n",
    "        if class_dictionary == None:\n",
    "            self.class_dictionary = {c:i for i, c in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_dictionary = class_dictionary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = train_files[index]\n",
    "        _, objects = parse_xml(annotations + filename + '.xml')\n",
    "        n_gt_boxes = len(objects)\n",
    "\n",
    "        image = load_image(images + filename +'.jpg')\n",
    "        image = image.convert(\"RGB\")\n",
    "        original_size = image.size\n",
    "        image = self.transformToResize(image)\n",
    "        new_size = image.size\n",
    "        image = self.transformToTensor(image, )\n",
    "        scale_x = new_size[0] / original_size[0]\n",
    "        scale_y = new_size[1] / original_size[1]\n",
    "\n",
    "        label_matrix = torch.zeros((self.split_size, self.split_size, self.n_bboxes, 5 + self.n_classes))\n",
    "        labeled_map = {}\n",
    "        for gt_box in range(n_gt_boxes):\n",
    "            cl = objects[gt_box]['class']\n",
    "            x, y, w, h = objects[gt_box]['bbox']\n",
    "            x = x * scale_x\n",
    "            y = y * scale_y\n",
    "            w = w * scale_x\n",
    "            h = h * scale_y\n",
    "\n",
    "            i, j = int(self.split_size * y), int(self.split_size * x)\n",
    "            x_cell, y_cell = self.split_size * x - j, self.split_size * y - i\n",
    "\n",
    "            w_cell, h_cell = (\n",
    "                w * self.split_size,\n",
    "                h * self.split_size,\n",
    "            )\n",
    "            for n_box in range(self.n_bboxes):\n",
    "                if label_matrix[i, j, n_box, 4] == 0:\n",
    "                    label_matrix[i, j, n_box] = torch.cat([torch.tensor([x_cell, y_cell, w_cell, h_cell, 1]),\n",
    "                                                           nn.functional.one_hot(torch.tensor(self.class_dictionary[cl]), num_classes=self.n_classes).float()])\n",
    "        return image, label_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66157934-0596-4b99-9646-a180b3996eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16 # 64 in original paper but resource exhausted error otherwise.\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 20\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "CHECKPOINT_PTH = 'checkpoint.pth'\n",
    "\n",
    "SPLIT_SIZE = 7\n",
    "N_BBOXES = 2\n",
    "N_CLASSES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc87febe-63bd-48b5-95ee-ad1683fa297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer,loss):\n",
    "    checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, CHECKPOINT_PTH)\n",
    "\n",
    "def load_checkpoint():\n",
    "    checkpoint = torch.load(CHECKPOINT_PTH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1344e99f-ea96-47f2-8e2e-77091b6562bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())\n",
    "        \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e942b9-3d65-46fb-8dc7-f8da736aaab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███▏                                          | 25/358 [02:56<44:40,  8.05s/it, loss=nan]"
     ]
    }
   ],
   "source": [
    "model = Yolo(split_size = SPLIT_SIZE, n_bboxes = N_BBOXES, n_classes = N_CLASSES)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "scheduler = CustomLRScheduler(optimizer= optimizer, lr_func= lr_scheduler)\n",
    "loss_fn = YoloLoss(split_size = SPLIT_SIZE, n_bboxes = N_BBOXES, n_classes = N_CLASSES)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "train_dataset = VOCDataset(data = train_files)\n",
    "val_dataset = VOCDataset(data = val_files)\n",
    "train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "for epoch in range(EPOCHS):\n",
    "    train(train_loader, model, optimizer, loss_fn)\n",
    "    scheduler.step(mean_avg_prec)\n",
    "\n",
    "checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "}\n",
    "save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf042a9c-8a1f-4bda-8850-49153d8e441f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[1;32m      3\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063fc66-836b-4667-a496-19b3a9b4ff0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
