{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f08777d1-da1b-4193-8f0b-b6cf5432b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ab9e750-36c5-4866-8558-b071310a56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "CHECKPOINT_PTH = 'checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d440a24-c744-47b3-b2fe-dcede76fe739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer,loss):\n",
    "    checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, CHECKPOINT_PTH)\n",
    "\n",
    "def load_checkpoint():\n",
    "    checkpoint = torch.load(CHECKPOINT_PTH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b1a1d5a-c7d6-4551-95c0-e2cff50cf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo(nn.Module):\n",
    "    def __init__(self, in_channels = 3, split_size = 7, n_bboxes = 2, n_classes = 20):\n",
    "        super().__init__()\n",
    "        self.conv_network = nn.Sequential(\n",
    "            self._conv_layer(in_channels, 64, 7, stride = 2),\n",
    "            self._get_max_pool(),\n",
    "            self._conv_layer(64, 192, 3),\n",
    "            self._get_max_pool(),\n",
    "            self._conv_layer(192, 128, 1),\n",
    "            self._conv_layer(128, 256, 3),\n",
    "            self._conv_layer(256, 256, 1),\n",
    "            self._conv_layer(256, 512, 3),\n",
    "            self._get_max_pool(),\n",
    "            self._get_four_conv_block_with_512_out(),\n",
    "            self._conv_layer(512, 512, 1),\n",
    "            self._conv_layer(512, 1024, 3),\n",
    "            self._get_max_pool(),\n",
    "            self._get_four_conv_block_with_1024_out(),\n",
    "            self._conv_layer(1024, 1024, 3),\n",
    "            self._conv_layer(1024, 1024, 3, stride = 2),\n",
    "            self._conv_layer(1024, 1024, 3),\n",
    "            self._conv_layer(1024, 1024, 3),\n",
    "        )\n",
    "        self.fc_network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024*split_size*split_size, 4906),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(4096, split_size * split_size * (n_bboxes * 5 + n_classes))\n",
    "        )\n",
    "\n",
    "    def _conv_layer(self,in_channels, out_channels, kernel_size, stride = 1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels= out_channels, kernel_size = kernel_size, stride = stride),\n",
    "            nn.LeakyReLU(negative_slope=0.1)\n",
    "        )\n",
    "    \n",
    "    def _get_max_pool(self):\n",
    "        return nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "    def _get_four_conv_block_with_512_out(self):\n",
    "        return nn.Sequential(\n",
    "            self._get_one_and_three_conv_with_512_out(), \n",
    "            self._get_one_and_three_conv_with_512_out(),\n",
    "            self._get_one_and_three_conv_with_512_out(),\n",
    "            self._get_one_and_three_conv_with_512_out()\n",
    "        )\n",
    "\n",
    "    def _get_one_and_three_conv_with_512_out(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1), \n",
    "            nn.Conv2d(256, 512, 3)\n",
    "        )\n",
    "\n",
    "    def _get_four_conv_block_with_1024_out(self):\n",
    "        return nn.Sequential(\n",
    "            self._get_one_and_three_with_1024_out(),\n",
    "            self._get_one_and_three_with_1024_out()\n",
    "        )\n",
    "        \n",
    "    def _get_one_and_three_with_1024_out(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, 1), \n",
    "            nn.Conv2d(512, 1024, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: (batch_size, image_len, image_width, channels)\n",
    "        returns: (batch_size, split_size, split_size, n_bboxes*5 + n_classes)\n",
    "        \"\"\"\n",
    "        conv_output = self.conv_network(inputs)\n",
    "        return self.fc_network(conv_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f44223a3-348f-46ef-921f-56aaae50ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1e-3 + (1e-2 - 1e-3) * (epoch / 10)  # Linearly increase from 1e-3 to 1e-2\n",
    "    elif epoch < 85:\n",
    "        return 1e-2  # Keep constant at 1e-2\n",
    "    elif epoch < 115:\n",
    "        return 1e-3  # Decrease to 1e-3\n",
    "    else:\n",
    "        return 1e-4  # Decrease to 1e-4\n",
    "\n",
    "\n",
    "class CustomLRScheduler:\n",
    "    def __init__(self, optimizer, lr_func):\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_func = lr_func\n",
    "\n",
    "    def step(self, epoch):\n",
    "        lr = self.lr_func(epoch)\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39f9243a-a24a-440c-bbfb-2c0c8271bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(box1, box2):\n",
    "    \"\"\"\n",
    "    box1: (batch_size_1, 5)\n",
    "    box2: (batch_size_2, 5)\n",
    "\n",
    "    return: (batch_size_1, Batch_size_2) float representing iou\n",
    "    \"\"\"\n",
    "    # Unpack the coordinates and dimensions\n",
    "    x1_center, y1_center, w1, h1 = box1[..., 0:4].T\n",
    "    x2_center, y2_center, w2, h2 = box2[..., 0:4].T\n",
    "\n",
    "    x1_min, y1_min = x1_center - w1 / 2, y1_center - h1 / 2\n",
    "    x1_max, y1_max = x1_center + w1 / 2, y1_center + h1 / 2\n",
    "    x2_min, y2_min = x2_center - w2 / 2, y2_center - h2 / 2\n",
    "    x2_max, y2_max = x2_center + w2 / 2, y2_center + h2 / 2\n",
    "\n",
    "    inter_xmin = torch.max(x1_min.unsqueeze(1), x2_min.unsqueeze(0))\n",
    "    inter_ymin = torch.max(y1_min.unsqueeze(1), y2_min.unsqueeze(0))\n",
    "    inter_xmax = torch.min(x1_max.unsqueeze(1), x2_max.unsqueeze(0))\n",
    "    inter_ymax = torch.min(y1_max.unsqueeze(1), y2_max.unsqueeze(0))\n",
    "\n",
    "    inter_width = torch.clamp(inter_xmax - inter_xmin, min=0)\n",
    "    inter_height = torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    union_area = box1_area.unsqueeze(1) + box2_area - inter_area\n",
    "\n",
    "    iou = inter_area / torch.clamp(union_area, min=1e-8)\n",
    "\n",
    "    return iou\n",
    "\n",
    "def non_max_suppression(boxes, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression (NMS) on the bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    boxes: shape (batch_size, 5)\n",
    "           Each row contains [x, y, w, h, score] coordinates of a bounding box.\n",
    "    iou_threshold: float\n",
    "                   IoU threshold for suppressing boxes.\n",
    "\n",
    "    Returns:\n",
    "    indices: list of int\n",
    "             Indices of the bounding boxes to keep.\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Convert boxes to numpy arrays (if not already)\n",
    "    boxes = torch.tensor(boxes)\n",
    "    scores = torch.tensor(scores)\n",
    "\n",
    "    # Compute the bottom-right coordinates and area of each box\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    w = boxes[:, 2]\n",
    "    h = boxes[:, 3]\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    areas = w * h\n",
    "\n",
    "    # Get the indices of the boxes sorted by scores in descending order\n",
    "    order = boxes[:, 4].argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        # The index of the current box with the highest score\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        # Compute the IoU of the kept box with the remaining boxes\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w_inter = np.maximum(0, xx2 - xx1)\n",
    "        h_inter = np.maximum(0, yy2 - yy1)\n",
    "        inter_area = w_inter * h_inter\n",
    "\n",
    "        iou = inter_area / (areas[i] + areas[order[1:]] - inter_area)\n",
    "\n",
    "        # Keep boxes with IoU less than the threshold\n",
    "        inds = np.where(iou <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e8b8e6c-f7df-496a-a8d3-1096831a35df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.tensor([[[1], [2], [3]],[[7],[8],[9]]])\n",
    "print(example.shape)\n",
    "example[...,0].unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6229935-a5ec-4e9c-a704-f1e81f67c98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, -1, 2], [1,2,3, 2], [3, 0, 4, 2]])\n",
    "b = torch.tensor([[3, 0, 4, 2],[3, 0, 4, 2]])\n",
    "\n",
    "intersection_over_union(a,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d6149af-b279-4369-a21b-c153b2018162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([[0,0,0,0]])\n",
    "(test == 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd99fe5a-6429-4513-8660-055fb2d94e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, split_size = 7, n_bboxes = 2, n_classes = 20, lambda_coord = 5, lambda_noobj = 0.5):\n",
    "        super().__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.split_size = split_size\n",
    "        self.n_bboxes = n_bboxes\n",
    "        self.n_classes = n_classes\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def assign_boxes(self, pred_boxes, gt_boxes):\n",
    "        \"\"\"\n",
    "        pred_boxes: (n_bboxes, 5)\n",
    "        gt_boxes: (n_gt_boxes, 5)\n",
    "        \"\"\"\n",
    "        iou_matrix = intersection_over_union(pred_boxes, gt_boxes) #(n_gt_boxes, n_bboxes)\n",
    "        \n",
    "        gt_assigned = torch.full((gt_boxes.size(0),), -1, dtype=torch.long)  # -1 means not assigned\n",
    "        pred_assigned = torch.full((pred_boxes.size(0),), -1, dtype=torch.long)\n",
    "\n",
    "        while iou_matrix.numel() > 0 and (iou_matrix != -1).any():\n",
    "\n",
    "            max_iou, max_iou_indices = iou_matrix.max(dim=1)  # Get max IoU for each gt_box\n",
    "            best_pred_idx = max_iou.argmax()  # The best predicted box index for the current max IoU\n",
    "            best_gt_idx = max_iou_indices[best_pred_idx]  # The ground truth box index for the best predicted box\n",
    "    \n",
    "            # Assign the best predicted box to the ground truth box\n",
    "            gt_assigned[best_gt_idx] = best_pred_idx\n",
    "            pred_assigned[best_pred_idx] = best_gt_idx\n",
    "    \n",
    "            # Invalidate the assigned row and column in the IoU matrix to prevent reassignment\n",
    "            iou_matrix[best_gt_idx, :] = -1\n",
    "            iou_matrix[:, best_pred_idx] = -1\n",
    "\n",
    "        return gt_assigned, pred_assigned\n",
    "\n",
    "    def get_gt_boxes(self, target):\n",
    "        object_exists = target[..., 4] == 1\n",
    "        gt_boxes = target[..., :4][object_exists]\n",
    "        gt_classes = target[..., 5:][object_exists]\n",
    "        return gt_boxes, gt_classes\n",
    "        \n",
    "    def forward(self, predictions, target):\n",
    "        \"\"\"\n",
    "        predictions: (batch_size, split_size, split_size, n_bboxes*5 + n_classes)\n",
    "        target: (batch_size, split_size, split_size, n_bboxes, (4+1+n_classes)) the last field in the target tells us if object exists or not\n",
    "        \"\"\"\n",
    "        batch_size = predictions.shape[0]\n",
    "        predictions = predictions.view(batch_size, self.split_size, self.split_size, self.n_bboxes*5 + self.n_classes)\n",
    "\n",
    "        pred_boxes = predictions[..., :self.n_bboxes * 5].view(batch_size, self.split_size, self.split_size, batch_size, self.B, 5)\n",
    "        pred_classes = predictions[..., self.n_bboxes * 5:]\n",
    "\n",
    "        gt_boxes, gt_classes = self.get_gt_boxes(target)\n",
    "\n",
    "        if gt_boxes.size(0) == 0:\n",
    "            return torch.tensor(0.0)\n",
    "\n",
    "        gt_assigned, pred_assigned = self.assign_boxes(pred_boxes.view(-1, 4), gt_boxes)\n",
    "\n",
    "        pred_box_coords = pred_boxes[..., :4]\n",
    "        pred_box_conf = pred_boxes[..., 4]\n",
    "        \n",
    "        gt_box_coords = gt_boxes[gt_assigned]\n",
    "        correct_pred = pred_assigned != -1\n",
    "        incorrect_pred = pred_assigned == -1\n",
    "\n",
    "        coord_loss = self.lambda_coord * (\n",
    "            self.mse_loss(pred_box_coords[..., :2], gt_box_coords[..., :2]) +\n",
    "            self.mse_loss(torch.sqrt(pred_box_coords[..., 2:]), torch.sqrt(gt_box_coords[..., 2:]))\n",
    "        )\n",
    "        object_loss = self.mse_loss(pred_box_conf[correct_pred], \n",
    "                                    torch.ones_like(pred_box_conf[correct_pred]))\n",
    "        \n",
    "        no_object_mask = (pred_assigned == -1).float()\n",
    "        no_object_loss = self.lambda_noobj * self.mse_loss(pred_box_conf[incorrect_pred], \n",
    "                                                           torch.zeros_like(pred_box_conf[incorrect_pred]))\n",
    "\n",
    "        class_loss = self.mse_loss(pred_classes, gt_classes)\n",
    "\n",
    "        total_loss = coord_loss + object_loss + no_object_loss + class_loss\n",
    "\n",
    "        return total_loss\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86a79a57-4355-4792-959c-aad45d4951f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pottedplant': 0, 'car': 1, 'chair': 2, 'cow': 3, 'bird': 4, 'aeroplane': 5, 'diningtable': 6, 'train': 7, 'bus': 8, 'sheep': 9, 'cat': 10, 'tvmonitor': 11, 'dog': 12, 'sofa': 13, 'horse': 14, 'person': 15, 'boat': 16, 'motorbike': 17, 'bicycle': 18, 'bottle': 19}\n"
     ]
    }
   ],
   "source": [
    "annotations = \"./VOCdevkit/VOC2012/Annotations/\"\n",
    "layout = \"./VOCdevkit/VOC2012/ImageSets/Main/\"\n",
    "images = \"./VOCdevkit/VOC2012/JPEGImages/\"\n",
    "def extract_filenames(train_file_path):\n",
    "    filenames = []\n",
    "    \n",
    "    with open(train_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if parts:\n",
    "                # The image ID is the first part of each line\n",
    "                filename = parts[0]\n",
    "                filenames.append(filename)\n",
    "    \n",
    "    return filenames\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.find('filename').text\n",
    "    objects_and_bboxes = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        # Extract object class\n",
    "        obj_class = obj.find('name').text\n",
    "\n",
    "        # Extract bounding box coordinates\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "        # Calculate x_center, y_center, width, height\n",
    "        x_center = (xmin + xmax) / 2\n",
    "        y_center = (ymin + ymax) / 2\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # Store object and bounding box information\n",
    "        objects_and_bboxes.append({\n",
    "            'class': obj_class,\n",
    "            'bbox': (x_center, y_center, width, height)\n",
    "        })\n",
    "\n",
    "    return filename, objects_and_bboxes\n",
    "def load_image(image_file):\n",
    "    return Image.open(image_file)\n",
    "    \n",
    "train_files = extract_filenames(layout + \"train.txt\")\n",
    "val_files = extract_filenames(layout + \"val.txt\")\n",
    "\n",
    "classes = set()\n",
    "for filename in train_files:\n",
    "    _, objects = parse_xml(annotations + filename + '.xml')\n",
    "    classes.update([c['class'] for c in objects])\n",
    "print({j:i for i, j in enumerate(classes)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f29e61-c31e-4dac-a967-a0a6d101391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data = train_files, files_dir = files_dir, split_size  7, n_bboxes = 2, n_classes = 3, class_dictionary = None, transform=None):\n",
    "        self.data = data\n",
    "        self.files_dir = files_dir\n",
    "        self.transform = transform\n",
    "        self.split_size = split_size\n",
    "        self.n_bboxes = n_bboxes\n",
    "        self.n_classes = n_classes\n",
    "        if class_dictionary == None:\n",
    "            self.class_dictionary = {j:i for i, clas in enumerate(classes)}\n",
    "        else:\n",
    "            self.class_dictionary = class_dictionary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
